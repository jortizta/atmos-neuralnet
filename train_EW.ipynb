{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.12.3"
    },
    "colab": {
      "name": "test2.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "epVhRIqboTS_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "9e9d8222-f22c-4e36-840a-7aaf42d37c2c"
      },
      "source": [
        "! git clone https://github.com/jortizta/atmos-neuralnet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'atmos-neuralnet'...\n",
            "remote: Enumerating objects: 2881, done.\u001b[K\n",
            "remote: Counting objects: 100% (2881/2881), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2866/2866), done.\u001b[K\n",
            "remote: Total 2881 (delta 21), reused 2862 (delta 12), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (2881/2881), 23.75 MiB | 10.24 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64D8OUxBh5rh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "674f7b7e-c871-4428-88e0-77bcf5e16422"
      },
      "source": [
        "cd atmos-neuralnet/src"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/atmos-neuralnet/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmQ8hsNIh_pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from net import TurbNetG, weights_init\n",
        "import os, sys, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import dataset\n",
        "import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns3KvQjniGt7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of training iterations\n",
        "iterations = 10000\n",
        "# batch size\n",
        "batch_size = 10\n",
        "# learning rate, generator\n",
        "lrG = 0.0006\n",
        "# decay learning rate?\n",
        "decayLr = True\n",
        "# channel exponent to control network size\n",
        "expo = 5\n",
        "# data set config\n",
        "prop=None # by default, use all from \"../data/train\"\n",
        "#prop=[1000,0.75,0,0.25] # mix data from multiple directories\n",
        "# save txt files with per epoch loss?\n",
        "saveL1 = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROM_MZbfiep6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoIter   = False\n",
        "dropout    = 0.\n",
        "doLoad     = \"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRBSQTyJiiV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "517205a1-ef21-40e5-c001-871f938ac735"
      },
      "source": [
        "seed = random.randint(0, 2**32 - 1)\n",
        "print(\"Random seed: {}\".format(seed))\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random seed: 4071311880\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBNfdV_6ij66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "c4fec068-8f18-46c6-8b66-dc57caf4c660"
      },
      "source": [
        "data = dataset.TurbDataset(shuffle=1)\n",
        "trainLoader = DataLoader(data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "print(\"Training batches: {}\".format(len(trainLoader)))\n",
        "dataValidation = dataset.ValiDataset(data)\n",
        "valiLoader = DataLoader(dataValidation, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "print(\"Validation batches: {}\".format(len(valiLoader)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training batches: 102\n",
            "Validation batches: 25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/atmos-neuralnet/src/dataset.py:69: RuntimeWarning: invalid value encountered in true_divide\n",
            "  rawDataNorm= rawDataF/sDev[np.newaxis,:,:]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xykB_yAimXX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0aae4b2-525f-42af-b8ad-37a21b647f94"
      },
      "source": [
        "# setup training\n",
        "epochs = int(iterations/len(trainLoader) + 0.5)\n",
        "netG = TurbNetG(channelExponent=expo, dropout=dropout)\n",
        "print(netG) # print full net\n",
        "model_parameters = filter(lambda p: p.requires_grad, netG.parameters())\n",
        "params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "print(\"Initialized TurbNet with {} trainable params \".format(params))\n",
        "\n",
        "netG.apply(weights_init)\n",
        "if len(doLoad)>0:\n",
        "    netG.load_state_dict(torch.load(doLoad))\n",
        "    print(\"Loaded model \"+doLoad)\n",
        "netG.cuda()\n",
        "\n",
        "criterionL1 = nn.L1Loss()\n",
        "criterionL1.cuda()\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lrG, betas=(0.5, 0.999), weight_decay=0.0)\n",
        "\n",
        "targets = Variable(torch.FloatTensor(batch_size, 2, 128, 1))\n",
        "inputs  = Variable(torch.FloatTensor(batch_size, 5, 128, 1))\n",
        "targets = targets.cuda()\n",
        "inputs  = inputs.cuda()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TurbNetG(\n",
            "  (layer1): Sequential(\n",
            "    (layer1_conv): Conv2d(5, 32, kernel_size=[4, 1], stride=(2, 2), padding=[1, 0])\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (layer2_leakyrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (layer2_conv): Conv2d(32, 64, kernel_size=[4, 1], stride=(2, 2), padding=[1, 0])\n",
            "    (layer2_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer2b): Sequential(\n",
            "    (layer2b_leakyrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (layer2b_conv): Conv2d(64, 64, kernel_size=[4, 1], stride=(2, 2), padding=[1, 0])\n",
            "    (layer2b_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (layer3_leakyrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (layer3_conv): Conv2d(64, 128, kernel_size=[4, 1], stride=(2, 2), padding=[1, 0])\n",
            "    (layer3_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (layer4_leakyrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (layer4_conv): Conv2d(128, 256, kernel_size=[2, 1], stride=(2, 2))\n",
            "    (layer4_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer5): Sequential(\n",
            "    (layer5_leakyrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (layer5_conv): Conv2d(256, 256, kernel_size=[2, 1], stride=(2, 2))\n",
            "    (layer5_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (layer6): Sequential(\n",
            "    (layer6_leakyrelu): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (layer6_conv): Conv2d(256, 256, kernel_size=[2, 1], stride=(2, 2))\n",
            "  )\n",
            "  (dlayer6): Sequential(\n",
            "    (dlayer6_relu): ReLU(inplace)\n",
            "    (dlayer6_upsam): Upsample(size=[2, 1], mode=nearest)\n",
            "    (dlayer6_tconv): Conv2d(256, 256, kernel_size=[1, 1], stride=(1, 1))\n",
            "    (dlayer6_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dlayer5): Sequential(\n",
            "    (dlayer5_relu): ReLU(inplace)\n",
            "    (dlayer5_upsam): Upsample(size=[4, 1], mode=nearest)\n",
            "    (dlayer5_tconv): Conv2d(512, 256, kernel_size=[1, 1], stride=(1, 1))\n",
            "    (dlayer5_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dlayer4): Sequential(\n",
            "    (dlayer4_relu): ReLU(inplace)\n",
            "    (dlayer4_upsam): Upsample(size=[8, 1], mode=nearest)\n",
            "    (dlayer4_tconv): Conv2d(512, 128, kernel_size=[3, 1], stride=(1, 1), padding=[1, 0])\n",
            "    (dlayer4_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dlayer3): Sequential(\n",
            "    (dlayer3_relu): ReLU(inplace)\n",
            "    (dlayer3_upsam): Upsample(size=[16, 1], mode=nearest)\n",
            "    (dlayer3_tconv): Conv2d(256, 64, kernel_size=[3, 1], stride=(1, 1), padding=[1, 0])\n",
            "    (dlayer3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dlayer2b): Sequential(\n",
            "    (dlayer2b_relu): ReLU(inplace)\n",
            "    (dlayer2b_upsam): Upsample(size=[32, 1], mode=nearest)\n",
            "    (dlayer2b_tconv): Conv2d(128, 64, kernel_size=[3, 1], stride=(1, 1), padding=[1, 0])\n",
            "    (dlayer2b_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dlayer2): Sequential(\n",
            "    (dlayer2_relu): ReLU(inplace)\n",
            "    (dlayer2_upsam): Upsample(size=[64, 1], mode=nearest)\n",
            "    (dlayer2_tconv): Conv2d(128, 32, kernel_size=[3, 1], stride=(1, 1), padding=[1, 0])\n",
            "    (dlayer2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (dlayer1): Sequential(\n",
            "    (dlayer1_relu): ReLU(inplace)\n",
            "    (dlayer1_tconv): ConvTranspose2d(64, 2, kernel_size=[4, 1], stride=(2, 2), padding=[1, 0])\n",
            "  )\n",
            ")\n",
            "Initialized TurbNet with 870402 trainable params \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N0YU7zklEHd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "94b4ca7e-25c1-475d-b557-e1e37bfddae7"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    print(\"Starting epoch {} / {}\".format((epoch+1),epochs))\n",
        "\n",
        "    netG.train()\n",
        "    L1_accum = 0.0\n",
        "    samples_accum = 0\n",
        "    for i, traindata in enumerate(trainLoader, 0):\n",
        "\n",
        "        inputs_cpu, targets_cpu = traindata\n",
        "        current_batch_size = targets_cpu.size(0)\n",
        "\n",
        "        targets_cpu, inputs_cpu = targets_cpu.float().cuda(), inputs_cpu.float().cuda()\n",
        "        inputs.data.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "        targets.data.resize_as_(targets_cpu).copy_(targets_cpu)\n",
        "\n",
        "        # compute LR decay\n",
        "        if decayLr:\n",
        "            currLr = utils.computeLR(epoch, epochs, lrG*0.1, lrG)\n",
        "            if currLr < lrG:\n",
        "                for g in optimizerG.param_groups:\n",
        "                    g['lr'] = currLr\n",
        "\n",
        "        netG.zero_grad()\n",
        "        gen_out = netG(inputs)\n",
        "\n",
        "        lossL1 = criterionL1(gen_out, targets)\n",
        "        lossL1.backward()\n",
        "\n",
        "        optimizerG.step()\n",
        "\n",
        "        lossL1viz = lossL1.item()\n",
        "        L1_accum += lossL1viz\n",
        "        samples_accum += current_batch_size\n",
        "\n",
        "\n",
        "\n",
        "        if i==len(trainLoader)-1:\n",
        "            logline = \"Epoch: {}, batch-idx: {}, L1: {}\\n\".format(epoch, i, lossL1viz)\n",
        "            #print(logline)\n",
        "\n",
        "\n",
        "    # validation\n",
        "    netG.eval()\n",
        "    L1val_accum = 0.0\n",
        "    for i, validata in enumerate(valiLoader, 0):\n",
        "        inputs_cpu, targets_cpu = validata\n",
        "        current_batch_size = targets_cpu.size(0)\n",
        "\n",
        "        targets_cpu, inputs_cpu = targets_cpu.float().cuda(), inputs_cpu.float().cuda()\n",
        "        inputs.data.resize_as_(inputs_cpu).copy_(inputs_cpu)\n",
        "        targets.data.resize_as_(targets_cpu).copy_(targets_cpu)\n",
        "\n",
        "        outputs = netG(inputs)\n",
        "        outputs_cpu = outputs.data.cpu().numpy()\n",
        "\n",
        "        lossL1 = criterionL1(outputs, targets)\n",
        "        L1val_accum += lossL1.item()\n",
        "\n",
        "\n",
        "        if i==0:\n",
        "            input_ndarray = inputs_cpu.cpu().numpy()[0]\n",
        "            v_norm = ( np.max(np.abs(input_ndarray[0,:,:]))**2 + np.max(np.abs(input_ndarray[1,:,:]))**2 )**0.5\n",
        "            #print(outputs[0]-targets[0])\n",
        "            #print(targets[0])\n",
        "\n",
        "            #outputs_denormalized = data.denormalize(outputs_cpu[0], v_norm)\n",
        "            #targets_denormalized = data.denormalize(targets_cpu.cpu().numpy()[0], v_norm)\n",
        "            #utils.makeDirs([\"results_train\"])\n",
        "            #utils.imageOut(\"results_train/epoch{}_{}\".format(epoch, i), outputs_denormalized, targets_denormalized, saveTargets=True)\n",
        "\n",
        "    # data for graph plotting\n",
        "    L1_accum    /= len(trainLoader)\n",
        "    L1val_accum /= len(valiLoader)\n",
        "    if saveL1:\n",
        "        if epoch==0:\n",
        "            utils.resetLog(prefix + \"L1.txt\"   )\n",
        "            utils.resetLog(prefix + \"L1val.txt\")\n",
        "        utils.log(prefix + \"L1.txt\"   , \"{} \".format(L1_accum), False)\n",
        "        utils.log(prefix + \"L1val.txt\", \"{} \".format(L1val_accum), False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1 / 98\n",
            "Starting epoch 2 / 98\n",
            "Starting epoch 3 / 98\n",
            "Starting epoch 4 / 98\n",
            "Starting epoch 5 / 98\n",
            "Starting epoch 6 / 98\n",
            "Starting epoch 7 / 98\n",
            "Starting epoch 8 / 98\n",
            "Starting epoch 9 / 98\n",
            "Starting epoch 10 / 98\n",
            "Starting epoch 11 / 98\n",
            "Starting epoch 12 / 98\n",
            "Starting epoch 13 / 98\n",
            "Starting epoch 14 / 98\n",
            "Starting epoch 15 / 98\n",
            "Starting epoch 16 / 98\n",
            "Starting epoch 17 / 98\n",
            "Starting epoch 18 / 98\n",
            "Starting epoch 19 / 98\n",
            "Starting epoch 20 / 98\n",
            "Starting epoch 21 / 98\n",
            "Starting epoch 22 / 98\n",
            "Starting epoch 23 / 98\n",
            "Starting epoch 24 / 98\n",
            "Starting epoch 25 / 98\n",
            "Starting epoch 26 / 98\n",
            "Starting epoch 27 / 98\n",
            "Starting epoch 28 / 98\n",
            "Starting epoch 29 / 98\n",
            "Starting epoch 30 / 98\n",
            "Starting epoch 31 / 98\n",
            "Starting epoch 32 / 98\n",
            "Starting epoch 33 / 98\n",
            "Starting epoch 34 / 98\n",
            "Starting epoch 35 / 98\n",
            "Starting epoch 36 / 98\n",
            "Starting epoch 37 / 98\n",
            "Starting epoch 38 / 98\n",
            "Starting epoch 39 / 98\n",
            "Starting epoch 40 / 98\n",
            "Starting epoch 41 / 98\n",
            "Starting epoch 42 / 98\n",
            "Starting epoch 43 / 98\n",
            "Starting epoch 44 / 98\n",
            "Starting epoch 45 / 98\n",
            "Starting epoch 46 / 98\n",
            "Starting epoch 47 / 98\n",
            "Starting epoch 48 / 98\n",
            "Starting epoch 49 / 98\n",
            "Starting epoch 50 / 98\n",
            "Starting epoch 51 / 98\n",
            "Starting epoch 52 / 98\n",
            "Starting epoch 53 / 98\n",
            "Starting epoch 54 / 98\n",
            "Starting epoch 55 / 98\n",
            "Starting epoch 56 / 98\n",
            "Starting epoch 57 / 98\n",
            "Starting epoch 58 / 98\n",
            "Starting epoch 59 / 98\n",
            "Starting epoch 60 / 98\n",
            "Starting epoch 61 / 98\n",
            "Starting epoch 62 / 98\n",
            "Starting epoch 63 / 98\n",
            "Starting epoch 64 / 98\n",
            "Starting epoch 65 / 98\n",
            "Starting epoch 66 / 98\n",
            "Starting epoch 67 / 98\n",
            "Starting epoch 68 / 98\n",
            "Starting epoch 69 / 98\n",
            "Starting epoch 70 / 98\n",
            "Starting epoch 71 / 98\n",
            "Starting epoch 72 / 98\n",
            "Starting epoch 73 / 98\n",
            "Starting epoch 74 / 98\n",
            "Starting epoch 75 / 98\n",
            "Starting epoch 76 / 98\n",
            "Starting epoch 77 / 98\n",
            "Starting epoch 78 / 98\n",
            "Starting epoch 79 / 98\n",
            "Starting epoch 80 / 98\n",
            "Starting epoch 81 / 98\n",
            "Starting epoch 82 / 98\n",
            "Starting epoch 83 / 98\n",
            "Starting epoch 84 / 98\n",
            "Starting epoch 85 / 98\n",
            "Starting epoch 86 / 98\n",
            "Starting epoch 87 / 98\n",
            "Starting epoch 88 / 98\n",
            "Starting epoch 89 / 98\n",
            "Starting epoch 90 / 98\n",
            "Starting epoch 91 / 98\n",
            "Starting epoch 92 / 98\n",
            "Starting epoch 93 / 98\n",
            "Starting epoch 94 / 98\n",
            "Starting epoch 95 / 98\n",
            "Starting epoch 96 / 98\n",
            "Starting epoch 97 / 98\n",
            "Starting epoch 98 / 98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPZ92_U2mEK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c2dce2c1-7c75-4b17-ff85-14c2516d3df2"
      },
      "source": [
        "ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset.py  progressbar.py  training_v0.ipynb  utils.py\n",
            "net.py      \u001b[0m\u001b[01;34m__pycache__\u001b[0m/    training_v0.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_vFmcUvnCUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}